# graph/llm_node.py - Ø°ÙƒÙŠ Ù…Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ

import sys
import os

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø³Ø§Ø±
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import TypedDict, Annotated
from config.config import load_system_prompt

class State(TypedDict):
    messages: Annotated[list, ...]

def llm_node(state: State, model):
    """Ø¹Ù‚Ø¯Ø© LLM Ø°ÙƒÙŠØ© Ù…Ø¹ Ø¨Ø­Ø« ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø©
        last_message = state["messages"][-1] if state["messages"] else ""
        user_input = ""
        
        if isinstance(last_message, tuple):
            user_input = last_message[1]
        elif hasattr(last_message, 'content'):
            user_input = last_message.content
        else:
            user_input = str(last_message)
        
        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªØ§Ø¬ Ø¨Ø­Ø«
        search_keywords = [
            'api', 'apis', 'endpoint', 'endpoints', 
            'authentication', 'auth', 'login', 'signin',
            'visa', 'travel', 'user', 'management',
            'Ø§Ø¨Ø­Ø«', 'Ø¨Ø­Ø«', 'Ø§Ø¨Ø­Ø«Ù„ÙŠ', 'Ø§Ø¸Ù‡Ø±', 'Ø§Ø¹Ø±Ø¶',
            'find', 'search', 'show', 'get', 'post', 'delete', 'put'
        ]
        
        needs_search = any(keyword.lower() in user_input.lower() for keyword in search_keywords)
        
        if needs_search:
            print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ù…Ø·Ù„ÙˆØ¨ Ù„Ù„Ø³Ø¤Ø§Ù„: {user_input}")
            
            # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø«
            try:
                from tools.chroma_search_tool import search_apis
                search_result = search_apis(user_input)
                
                # prompt Ù…Ø­Ø³Ù† Ù…Ø¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«
                enhanced_prompt = f"""
Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø³Ø£Ù„: {user_input}

Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
{search_result}

Ù‚Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø£Ø¹Ù„Ø§Ù‡.
Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©.
"""
                
                # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LLM Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                messages = [("system", load_system_prompt()), ("user", enhanced_prompt)]
                response = model.invoke(messages)
                
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
                # Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø§Ø¯ÙŠØ© Ù„Ùˆ ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø«
                messages = [("system", load_system_prompt())] + state["messages"]
                response = model.invoke(messages)
        else:
            print(f"ğŸ’¬ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø§Ø¯ÙŠØ© Ù„Ù„Ø³Ø¤Ø§Ù„: {user_input}")
            
            # Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø§Ø¯ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø¨Ø­Ø«
            messages = [("system", load_system_prompt())] + state["messages"]
            response = model.invoke(messages)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        if hasattr(response, 'content'):
            return {"messages": [("assistant", response.content)]}
        else:
            return {"messages": [("assistant", str(response))]}
        
    except Exception as e:
        print(f"âŒ Error in llm_node: {e}")
        return {"messages": [("assistant", f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")]}

# Ø§Ø®ØªØ¨Ø§Ø±
if __name__ == "__main__":
    print("ğŸ§ª Testing smart llm_node...")
    try:
        prompt = load_system_prompt()
        print(f"âœ… System prompt loaded: {len(prompt)} characters")
        
        from tools.chroma_search_tool import search_apis
        result = search_apis("test")
        print(f"âœ… Search function works: {len(result)} characters")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")